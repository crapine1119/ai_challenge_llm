# src/infrastructure/prompt/manager.py
import hashlib
import json
import os
import re
from typing import Any, Dict, Optional, List, Tuple

import yaml
from langchain.prompts import PromptTemplate
from langchain.prompts.chat import ChatPromptTemplate

from infrastructure.db.database import get_session
from infrastructure.db.models import Prompt as PromptORM
from infrastructure.prompt.repository import PromptRepository
from infrastructure.prompt.schema import PromptFile
from infrastructure.prompt.schema import PromptTemplateInput  # PromptTemplateInput ì‚¬ìš©
from infrastructure.prompt.sync import fast_sync_one, _resolve_path

FAST_SYNC_ON_STALE = os.getenv("PROMPT_FAST_SYNC", "1").lower() in ("1", "true", "yes", "on")


# íŒŒì¼ ì–´ë””ë“ (ëª¨ë“ˆ ì „ì—­) ë³´ì¡° í•¨ìˆ˜ ì¶”ê°€
_JINJA_VAR = re.compile(r"\{\{\s*(\w+)\s*\}\}")


def _jinja_to_langchain(text: str) -> str:
    # {{ var }} -> {var}
    return _JINJA_VAR.sub(r"{\1}", text)


class PromptRenderResult(Dict[str, Any]):
    """
    ë°˜í™˜ í‚¤:
      - system: Optional[str]            # ë‚´ë¶€ í˜¸í™˜ í‚¤
      - user_text: str                   # ë‚´ë¶€ í˜¸í™˜ í‚¤
      - json_schema_key: Optional[str]
      - params: Dict[str, Any]
      - key: str
      - version: str
      - language: Optional[str]
      - prompt_id: Optional[int]         # âœ… í”„ë¡¬í”„íŠ¸ FK (prompts.id)

      # ì™¸ë¶€ ì„œë¹„ìŠ¤(JDGenerationService ë“±) í˜¸í™˜ ë³„ì¹­:
      - system_prompt: Optional[str]     # = system
      - prompt: str                      # = user_text
    """


def _split_system_user_from_messages(messages: List[Dict[str, str]]) -> Tuple[Optional[str], str]:
    sys_parts, user_parts = [], []
    for m in messages:
        role = (m.get("role") or "").lower()
        content = m.get("content") or ""
        if role == "system":
            sys_parts.append(content)
        elif role in ["user", "human"]:
            user_parts.append(content)
        # assistant/tool ì—­í• ì€ í˜„ì¬ ë Œë” ê²°ê³¼ì—ì„œ ì œì™¸
    system = "\n".join(sys_parts) if sys_parts else None
    user_text = "\n\n".join(user_parts)
    return system, user_text


def _ensure_required_vars(required: List[str], ctx: Dict[str, Any]) -> None:
    missing = [k for k in (required or []) if k not in ctx]
    if missing:
        raise KeyError(f"Missing prompt variables: {missing}")


async def render_by_key_version(
    *,
    key: str,
    version: str,
    language: Optional[str],
    context: Dict[str, Any],
) -> PromptRenderResult:
    """
    DBì—ì„œ (key, version, language) í”„ë¡¬í”„íŠ¸ë¥¼ ê°€ì ¸ì™€ LangChain í…œí”Œë¦¿ìœ¼ë¡œ ë Œë”í•©ë‹ˆë‹¤.
    - string í…œí”Œë¦¿: PromptTemplate
    - chat í…œí”Œë¦¿: ChatPromptTemplate â†’ system/user ë¶„ë¦¬
    """
    # âœ… ë Œë” ì§ì „ ë³€ê²½ ê°ì§€ & ë‹¨ê±´ ë™ê¸°í™”
    await _maybe_fast_sync_before_render(key=key, version=version, language=language)

    async for session in get_session():
        repo = PromptRepository(session)
        p: Optional[PromptORM] = await repo.get(key=key, version=version, language=language)
        if not p or not p.is_active:
            raise LookupError(f"Prompt not found or inactive: {key}/{version} (lang={language})")

        # í•„ìˆ˜ ë³€ìˆ˜ ì²´í¬
        _ensure_required_vars(p.required_vars or [], context)

        # string í…œí”Œë¦¿
        if p.prompt_type == "string":
            tmpl_src = _jinja_to_langchain(p.template or "")
            tmpl = PromptTemplate.from_template(tmpl_src)
            user_text = tmpl.format(**context)
            return PromptRenderResult(
                system=None,
                user_text=user_text,
                json_schema_key=p.json_schema_key,
                params=p.params or {},
                language=language,
                key=key,
                version=version,
            )

        # chat í…œí”Œë¦¿
        if p.prompt_type == "chat":
            msgs = p.messages or []
            # Jinja í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ LangChain ìŠ¤íƒ€ì¼ë¡œ ë³€í™˜
            normalized = [(m["role"], _jinja_to_langchain(m["content"])) for m in msgs]
            chat = ChatPromptTemplate.from_messages(normalized)
            rendered_msgs = chat.format_messages(**context)
            serial = [{"role": m.type, "content": m.content} for m in rendered_msgs]
            system, user_text = _split_system_user_from_messages(serial)
            return PromptRenderResult(
                system=system,
                user_text=user_text,
                json_schema_key=p.json_schema_key,
                params=p.params or {},
                language=language,
                key=key,
                version=version,
                prompt_id=p.id,
            )

        raise ValueError(f"Unsupported prompt_type: {p.prompt_type}")


async def render_by_style(
    *,
    style_name: Optional[str],
    fallback_key: str,
    fallback_ver: str,
    language: Optional[str],
    context: Dict[str, Any],
) -> PromptRenderResult:
    # ğŸ” ê³¼ê±°ì—ëŠ” style_name â†’ jd_styles â†’ prompt_key/version ë§¤í•‘ì„ í–ˆìŒ
    # ì´ì œëŠ” prompt ë§¤í•‘ì„ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ í•­ìƒ fallbackìœ¼ë¡œ ë Œë”
    return await render_by_key_version(key=fallback_key, version=fallback_ver, language=language, context=context)


# ------------------------------------------------------------------------------
# High-level Manager
# ------------------------------------------------------------------------------


class PromptManager:
    """
    ì„œë¹„ìŠ¤ ë ˆì´ì–´ì—ì„œ ì‚¬ìš©í•˜ê¸° ì¢‹ì€ ê³ ìˆ˜ì¤€ API.
    - render_chat(prompt_input): PromptTemplateInput â†’ PromptRenderResult
    """

    async def render_chat(self, prompt_input: PromptTemplateInput) -> PromptRenderResult:
        """
        PromptTemplateInputì„ ë°›ì•„ í”„ë¡¬í”„íŠ¸ë¥¼ ë Œë”í•©ë‹ˆë‹¤.
        style_nameì´ ì£¼ì–´ì§€ë©´ ìŠ¤íƒ€ì¼ ìš°ì„ , ì•„ë‹ˆë©´ (key, version)ë¡œ ì§ì ‘ ë Œë”.
        """
        if prompt_input.style_name:
            return await render_by_style(
                style_name=prompt_input.style_name,
                fallback_key=prompt_input.prompt_key,
                fallback_ver=prompt_input.prompt_version,
                language=prompt_input.language,
                context=prompt_input.variables or {},
            )
        return await render_by_key_version(
            key=prompt_input.prompt_key,
            version=prompt_input.prompt_version,
            language=prompt_input.language,
            context=prompt_input.variables or {},
        )


async def _maybe_fast_sync_before_render(key: str, version: str, language: Optional[str]) -> None:
    """
    ë¹ ë¥¸ ë³€ê²½ ê°ì§€:
      - íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ íŒŒì¼ì˜ hashì™€ DBì˜ content_hash ë¹„êµ â†’ ë‹¤ë¥´ë©´ ê·¸ í”„ë¡¬í”„íŠ¸ë§Œ upsert
      - íŒŒì¼ì´ ì—†ìœ¼ë©´ ì•„ë¬´ ê²ƒë„ ì•ˆ í•¨
    """
    if not FAST_SYNC_ON_STALE:
        return
    path = _resolve_path(key, version, language)
    if not path:
        return  # íŒŒì¼ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ

    # íŒŒì¼ hash ê³„ì‚°

    data = yaml.safe_load(path.read_text(encoding="utf-8")) or {}
    # í•´ì‹œ ì…ë ¥ê°’ì€ sync.pyì™€ ë™ì¼ êµ¬ì¡°ë¡œ

    pf = PromptFile(**data)
    src = {
        "prompt_type": pf.prompt_type,
        "messages": pf.messages,
        "template": pf.template,
        "params": pf.params or {},
        "json_schema_key": pf.json_schema_key,
        "required_vars": pf.required_vars or [],
    }
    s = json.dumps(src, ensure_ascii=False, sort_keys=True, separators=(",", ":"))
    fhash = hashlib.sha256(s.encode("utf-8")).hexdigest()

    # DB content_hash ì¡°íšŒ
    async for session in get_session():
        repo = PromptRepository(session)
        row: Optional[PromptORM] = await repo.get(key=key, version=version, language=language)
        db_hash = getattr(row, "content_hash", None) if row else None

    # ë‹¤ë¥´ë©´ ë‹¨ê±´ fast sync ì‹¤í–‰
    if fhash != db_hash:
        await fast_sync_one(key, version, language)
